\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}              &   calmarRatio    & \textbf{  R-squared:         } &     0.752   \\
\textbf{Model:}                      &       OLS        & \textbf{  Adj. R-squared:    } &     0.686   \\
\textbf{Method:}                     &  Least Squares   & \textbf{  F-statistic:       } &     11.28   \\
\textbf{Date:}                       & Fri, 20 May 2022 & \textbf{  Prob (F-statistic):} &  3.11e-11   \\
\textbf{Time:}                       &     10:03:43     & \textbf{  Log-Likelihood:    } &   -15.814   \\
\textbf{No. Observations:}           &          67      & \textbf{  AIC:               } &     61.63   \\
\textbf{Df Residuals:}               &          52      & \textbf{  BIC:               } &     94.70   \\
\textbf{Df Model:}                   &          14      & \textbf{                     } &             \\
\textbf{Covariance Type:}            &    nonrobust     & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
                                     & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                   &       0.9795  &        0.116     &     8.463  &         0.000        &        0.747    &        1.212     \\
\textbf{reward[T.sharpeRatio]}       &      -0.0750  &        0.114     &    -0.660  &         0.512        &       -0.303    &        0.153     \\
\textbf{epochs}                      &       0.5322  &        0.065     &     8.199  &         0.000        &        0.402    &        0.662     \\
\textbf{gamma}                       &       0.0546  &        0.053     &     1.038  &         0.304        &       -0.051    &        0.160     \\
\textbf{learning\_rate}              &       0.0416  &        0.045     &     0.921  &         0.361        &       -0.049    &        0.132     \\
\textbf{max\_memo}                   &      -0.0446  &        0.051     &    -0.878  &         0.384        &       -0.147    &        0.057     \\
\textbf{net\_dim}                    &      -0.1580  &        0.052     &    -3.059  &         0.004        &       -0.262    &       -0.054     \\
\textbf{target\_step}                &       0.1000  &        0.051     &     1.966  &         0.055        &       -0.002    &        0.202     \\
\textbf{tau}                         &      -0.0334  &        0.054     &    -0.624  &         0.535        &       -0.141    &        0.074     \\
\textbf{np.power(net\_dim, 2)}       &      -0.5405  &        0.081     &    -6.689  &         0.000        &       -0.703    &       -0.378     \\
\textbf{np.power(max\_memo, 2)}      &       0.0479  &        0.048     &     1.007  &         0.319        &       -0.048    &        0.143     \\
\textbf{np.power(gamma, 2)}          &      -0.0010  &        0.051     &    -0.020  &         0.984        &       -0.103    &        0.101     \\
\textbf{np.power(tau, 2)}            &      -0.1102  &        0.055     &    -2.019  &         0.049        &       -0.220    &       -0.001     \\
\textbf{np.power(learning\_rate, 2)} &      -0.0863  &        0.050     &    -1.726  &         0.090        &       -0.187    &        0.014     \\
\textbf{np.power(target\_step, 2)}   &      -0.1340  &        0.046     &    -2.943  &         0.005        &       -0.225    &       -0.043     \\
\textbf{np.power(epochs, 2)}         &       0.5514  &        0.072     &     7.613  &         0.000        &        0.406    &        0.697     \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 27.342 & \textbf{  Durbin-Watson:     } &    1.791  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &   67.099  \\
\textbf{Skew:}          &  1.246 & \textbf{  Prob(JB):          } & 2.69e-15  \\
\textbf{Kurtosis:}      &  7.222 & \textbf{  Cond. No.          } & 2.89e+16  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

Notes: \newline
 [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \newline
 [2] The smallest eigenvalue is 7.54e-31. This might indicate that there are \newline
 strong multicollinearity problems or that the design matrix is singular.