\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}              &    cumReturn     & \textbf{  R-squared:         } &     0.107   \\
\textbf{Model:}                      &       OLS        & \textbf{  Adj. R-squared:    } &     0.061   \\
\textbf{Method:}                     &  Least Squares   & \textbf{  F-statistic:       } &     2.328   \\
\textbf{Date:}                       & Sat, 21 May 2022 & \textbf{  Prob (F-statistic):} &  0.00374    \\
\textbf{Time:}                       &     12:09:18     & \textbf{  Log-Likelihood:    } &    79.000   \\
\textbf{No. Observations:}           &         308      & \textbf{  AIC:               } &    -126.0   \\
\textbf{Df Residuals:}               &         292      & \textbf{  BIC:               } &    -66.32   \\
\textbf{Df Model:}                   &          15      & \textbf{                     } &             \\
\textbf{Covariance Type:}            &    nonrobust     & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
                                     & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                   &       0.0108  &        0.039     &     0.279  &         0.781        &       -0.065    &        0.087     \\
\textbf{source[T.Twitter]}           &      -0.0230  &        0.023     &    -1.004  &         0.316        &       -0.068    &        0.022     \\
\textbf{dropout}                     &       0.0021  &        0.012     &     0.178  &         0.859        &       -0.021    &        0.025     \\
\textbf{epochs}                      &      -0.0206  &        0.012     &    -1.659  &         0.098        &       -0.045    &        0.004     \\
\textbf{learning\_rate}              &      -0.0317  &        0.011     &    -2.809  &         0.005        &       -0.054    &       -0.009     \\
\textbf{linear\_layers}              &      -0.0061  &        0.012     &    -0.509  &         0.611        &       -0.030    &        0.017     \\
\textbf{rnn\_layers}                 &      -0.0029  &        0.011     &    -0.250  &         0.803        &       -0.025    &        0.020     \\
\textbf{use\_attention}              &       0.0046  &        0.011     &     0.412  &         0.680        &       -0.017    &        0.027     \\
\textbf{use\_price}                  &      -0.0138  &        0.011     &    -1.224  &         0.222        &       -0.036    &        0.008     \\
\textbf{window\_size}                &       0.0096  &        0.011     &     0.855  &         0.393        &       -0.012    &        0.032     \\
\textbf{np.power(rnn\_layers, 2)}    &       0.0108  &        0.016     &     0.682  &         0.496        &       -0.020    &        0.042     \\
\textbf{np.power(window\_size, 2)}   &       0.0322  &        0.015     &     2.132  &         0.034        &        0.002    &        0.062     \\
\textbf{np.power(linear\_layers, 2)} &      -0.0299  &        0.017     &    -1.729  &         0.085        &       -0.064    &        0.004     \\
\textbf{np.power(learning\_rate, 2)} &       0.0260  &        0.013     &     1.989  &         0.048        &        0.000    &        0.052     \\
\textbf{np.power(epochs, 2)}         &      -0.0027  &        0.013     &    -0.205  &         0.838        &       -0.029    &        0.023     \\
\textbf{np.power(dropout, 2)}        &      -0.0235  &        0.013     &    -1.760  &         0.079        &       -0.050    &        0.003     \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       &  4.107 & \textbf{  Durbin-Watson:     } &    0.552  \\
\textbf{Prob(Omnibus):} &  0.128 & \textbf{  Jarque-Bera (JB):  } &    4.421  \\
\textbf{Skew:}          &  0.143 & \textbf{  Prob(JB):          } &    0.110  \\
\textbf{Kurtosis:}      &  3.512 & \textbf{  Cond. No.          } &     10.6  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

Notes: \newline
 [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.