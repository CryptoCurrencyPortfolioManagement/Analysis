\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}                   &   sortinoRatio   & \textbf{  R-squared:         } &     0.224   \\
\textbf{Model:}                           &       OLS        & \textbf{  Adj. R-squared:    } &    -0.018   \\
\textbf{Method:}                          &  Least Squares   & \textbf{  F-statistic:       } &    0.9272   \\
\textbf{Date:}                            & Sat, 05 Feb 2022 & \textbf{  Prob (F-statistic):} &    0.554    \\
\textbf{Time:}                            &     10:20:47     & \textbf{  Log-Likelihood:    } &   -950.14   \\
\textbf{No. Observations:}                &          81      & \textbf{  AIC:               } &     1940.   \\
\textbf{Df Residuals:}                    &          61      & \textbf{  BIC:               } &     1988.   \\
\textbf{Df Model:}                        &          19      & \textbf{                     } &             \\
\textbf{Covariance Type:}                 &    nonrobust     & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
                                          & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                        &   -6992.0975  &     1.35e+04     &    -0.519  &         0.605        &    -3.39e+04    &     1.99e+04     \\
\textbf{source[T.Twitter]}                &    1.795e+04  &     9225.591     &     1.945  &         0.056        &     -500.453    &     3.64e+04     \\
\textbf{dropout}                          &    3965.6956  &     6185.505     &     0.641  &         0.524        &    -8402.987    &     1.63e+04     \\
\textbf{epochs}                           &   -6374.4295  &     6270.213     &    -1.017  &         0.313        &    -1.89e+04    &     6163.636     \\
\textbf{learning\_rate}                   &    2173.4994  &     4551.613     &     0.478  &         0.635        &    -6928.013    &     1.13e+04     \\
\textbf{linear\_layers}                   &   -8985.7537  &     6580.625     &    -1.365  &         0.177        &    -2.21e+04    &     4173.020     \\
\textbf{n\_head}                          &    5712.4538  &     6145.992     &     0.929  &         0.356        &    -6577.216    &      1.8e+04     \\
\textbf{rnn\_layers}                      &    7173.5884  &     6589.045     &     1.089  &         0.281        &    -6002.020    &     2.03e+04     \\
\textbf{transformer\_layers}              &    2879.5819  &     5908.741     &     0.487  &         0.628        &    -8935.675    &     1.47e+04     \\
\textbf{use\_attention}                   &   -2917.1070  &     4562.400     &    -0.639  &         0.525        &     -1.2e+04    &     6205.976     \\
\textbf{use\_price}                       &    2598.0086  &     4520.682     &     0.575  &         0.568        &    -6441.653    &     1.16e+04     \\
\textbf{window\_size}                     &   -5166.8051  &     4713.016     &    -1.096  &         0.277        &    -1.46e+04    &     4257.452     \\
\textbf{np.power(epochs, 2)}              &     364.8917  &     4663.595     &     0.078  &         0.938        &    -8960.543    &     9690.327     \\
\textbf{np.power(transformer\_layers, 2)} &    4211.5156  &     6098.374     &     0.691  &         0.492        &    -7982.938    &     1.64e+04     \\
\textbf{np.power(n\_head, 2)}             &   -1577.6383  &     4363.390     &    -0.362  &         0.719        &    -1.03e+04    &     7147.499     \\
\textbf{np.power(rnn\_layers, 2)}         &     692.3620  &     5984.378     &     0.116  &         0.908        &    -1.13e+04    &     1.27e+04     \\
\textbf{np.power(dropout, 2)}             &    1429.1222  &     4407.448     &     0.324  &         0.747        &    -7384.115    &     1.02e+04     \\
\textbf{np.power(linear\_layers, 2)}      &    5205.5715  &     5016.077     &     1.038  &         0.303        &    -4824.695    &     1.52e+04     \\
\textbf{np.power(window\_size, 2)}        &   -2808.6004  &     6603.094     &    -0.425  &         0.672        &     -1.6e+04    &     1.04e+04     \\
\textbf{np.power(learning\_rate, 2)}      &   -4193.3972  &     5055.811     &    -0.829  &         0.410        &    -1.43e+04    &     5916.322     \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 138.434 & \textbf{  Durbin-Watson:     } &    1.973  \\
\textbf{Prob(Omnibus):} &   0.000 & \textbf{  Jarque-Bera (JB):  } & 7197.230  \\
\textbf{Skew:}          &   5.916 & \textbf{  Prob(JB):          } &     0.00  \\
\textbf{Kurtosis:}      &  47.638 & \textbf{  Cond. No.          } &     12.7  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

Notes: \newline
 [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.