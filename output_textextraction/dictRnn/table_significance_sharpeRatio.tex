\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}              &   sharpeRatio    & \textbf{  R-squared:         } &     0.612   \\
\textbf{Model:}                      &       OLS        & \textbf{  Adj. R-squared:    } &     0.587   \\
\textbf{Method:}                     &  Least Squares   & \textbf{  F-statistic:       } &     24.06   \\
\textbf{Date:}                       & Sat, 21 May 2022 & \textbf{  Prob (F-statistic):} &  4.04e-51   \\
\textbf{Time:}                       &     11:53:44     & \textbf{  Log-Likelihood:    } &    35.241   \\
\textbf{No. Observations:}           &         326      & \textbf{  AIC:               } &    -28.48   \\
\textbf{Df Residuals:}               &         305      & \textbf{  BIC:               } &     51.04   \\
\textbf{Df Model:}                   &          20      & \textbf{                     } &             \\
\textbf{Covariance Type:}            &    nonrobust     & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
                                     & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                   &       0.8523  &        0.046     &    18.622  &         0.000        &        0.762    &        0.942     \\
\textbf{dicts[T.LM+Vader]}           &      -0.1504  &        0.057     &    -2.643  &         0.009        &       -0.262    &       -0.038     \\
\textbf{dicts[T.Vader]}              &       0.0085  &        0.032     &     0.265  &         0.791        &       -0.054    &        0.071     \\
\textbf{dicts[T.bert]}               &       0.0203  &        0.044     &     0.463  &         0.644        &       -0.066    &        0.107     \\
\textbf{dicts[T.bert+LM]}            &      -0.1805  &        0.062     &    -2.916  &         0.004        &       -0.302    &       -0.059     \\
\textbf{dicts[T.bert+LM+Vader]}      &      -0.2266  &        0.064     &    -3.521  &         0.000        &       -0.353    &       -0.100     \\
\textbf{source[T.Twitter]}           &      -0.1613  &        0.038     &    -4.201  &         0.000        &       -0.237    &       -0.086     \\
\textbf{dropout}                     &      -0.0583  &        0.022     &    -2.688  &         0.008        &       -0.101    &       -0.016     \\
\textbf{epochs}                      &       0.0467  &        0.025     &     1.865  &         0.063        &       -0.003    &        0.096     \\
\textbf{learning\_rate}              &       0.0213  &        0.019     &     1.106  &         0.270        &       -0.017    &        0.059     \\
\textbf{linear\_layers}              &      -0.0078  &        0.016     &    -0.482  &         0.630        &       -0.040    &        0.024     \\
\textbf{rnn\_layers}                 &       0.0092  &        0.013     &     0.686  &         0.493        &       -0.017    &        0.036     \\
\textbf{use\_attention}              &       0.0145  &        0.013     &     1.116  &         0.265        &       -0.011    &        0.040     \\
\textbf{use\_price}                  &       0.0340  &        0.013     &     2.553  &         0.011        &        0.008    &        0.060     \\
\textbf{window\_size}                &       0.0537  &        0.017     &     3.203  &         0.002        &        0.021    &        0.087     \\
\textbf{np.power(rnn\_layers, 2)}    &      -0.0229  &        0.017     &    -1.331  &         0.184        &       -0.057    &        0.011     \\
\textbf{np.power(window\_size, 2)}   &       0.0336  &        0.016     &     2.134  &         0.034        &        0.003    &        0.065     \\
\textbf{np.power(linear\_layers, 2)} &       0.0022  &        0.018     &     0.120  &         0.904        &       -0.033    &        0.038     \\
\textbf{np.power(learning\_rate, 2)} &      -0.0492  &        0.011     &    -4.325  &         0.000        &       -0.072    &       -0.027     \\
\textbf{np.power(epochs, 2)}         &      -0.0424  &        0.018     &    -2.364  &         0.019        &       -0.078    &       -0.007     \\
\textbf{np.power(dropout, 2)}        &      -0.0161  &        0.012     &    -1.332  &         0.184        &       -0.040    &        0.008     \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 18.602 & \textbf{  Durbin-Watson:     } &    1.157  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &   36.016  \\
\textbf{Skew:}          & -0.309 & \textbf{  Prob(JB):          } & 1.51e-08  \\
\textbf{Kurtosis:}      &  4.507 & \textbf{  Cond. No.          } &     21.0  \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

Notes: \newline
 [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.