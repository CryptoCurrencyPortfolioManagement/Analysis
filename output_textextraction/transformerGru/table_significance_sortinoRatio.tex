\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}                   &   sortinoRatio   & \textbf{  R-squared:         } &     0.067   \\
\textbf{Model:}                           &       OLS        & \textbf{  Adj. R-squared:    } &     0.004   \\
\textbf{Method:}                          &  Least Squares   & \textbf{  F-statistic:       } &     1.060   \\
\textbf{Date:}                            & Sat, 05 Mar 2022 & \textbf{  Prob (F-statistic):} &    0.393    \\
\textbf{Time:}                            &     18:20:40     & \textbf{  Log-Likelihood:    } &   -1929.0   \\
\textbf{No. Observations:}                &         302      & \textbf{  AIC:               } &     3898.   \\
\textbf{Df Residuals:}                    &         282      & \textbf{  BIC:               } &     3972.   \\
\textbf{Df Model:}                        &          19      & \textbf{                     } &             \\
\textbf{Covariance Type:}                 &    nonrobust     & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
                                          & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                        &       5.9768  &       28.915     &     0.207  &         0.836        &      -50.940    &       62.894     \\
\textbf{source[T.Twitter]}                &     -33.6351  &       21.856     &    -1.539  &         0.125        &      -76.656    &        9.386     \\
\textbf{dropout}                          &       6.0859  &        9.453     &     0.644  &         0.520        &      -12.522    &       24.694     \\
\textbf{epochs}                           &      10.7771  &       10.317     &     1.045  &         0.297        &       -9.532    &       31.086     \\
\textbf{learning\_rate}                   &      13.9409  &       13.735     &     1.015  &         0.311        &      -13.096    &       40.978     \\
\textbf{linear\_layers}                   &      -4.0050  &       10.786     &    -0.371  &         0.711        &      -25.236    &       17.226     \\
\textbf{n\_head}                          &     -12.0008  &       12.563     &    -0.955  &         0.340        &      -36.730    &       12.728     \\
\textbf{rnn\_layers}                      &       6.2253  &       14.915     &     0.417  &         0.677        &      -23.134    &       35.585     \\
\textbf{transformer\_layers}              &      -3.6353  &       12.295     &    -0.296  &         0.768        &      -27.836    &       20.565     \\
\textbf{use\_attention}                   &      -7.3037  &        9.288     &    -0.786  &         0.432        &      -25.587    &       10.979     \\
\textbf{use\_price}                       &     -13.2007  &        9.282     &    -1.422  &         0.156        &      -31.472    &        5.071     \\
\textbf{window\_size}                     &       8.3734  &        9.316     &     0.899  &         0.370        &       -9.964    &       26.711     \\
\textbf{np.power(dropout, 2)}             &      -5.4663  &        8.831     &    -0.619  &         0.536        &      -22.850    &       11.917     \\
\textbf{np.power(epochs, 2)}              &     -10.6822  &        8.813     &    -1.212  &         0.226        &      -28.029    &        6.664     \\
\textbf{np.power(n\_head, 2)}             &       8.0221  &        9.287     &     0.864  &         0.388        &      -10.259    &       26.303     \\
\textbf{np.power(rnn\_layers, 2)}         &      -3.7886  &       11.057     &    -0.343  &         0.732        &      -25.553    &       17.976     \\
\textbf{np.power(learning\_rate, 2)}      &      22.8572  &       11.974     &     1.909  &         0.057        &       -0.712    &       46.426     \\
\textbf{np.power(transformer\_layers, 2)} &       2.7929  &       11.415     &     0.245  &         0.807        &      -19.677    &       25.263     \\
\textbf{np.power(window\_size, 2)}        &       2.1298  &       10.351     &     0.206  &         0.837        &      -18.246    &       22.505     \\
\textbf{np.power(linear\_layers, 2)}      &      -1.2114  &       12.926     &    -0.094  &         0.925        &      -26.655    &       24.232     \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 662.344 & \textbf{  Durbin-Watson:     } &     2.062   \\
\textbf{Prob(Omnibus):} &   0.000 & \textbf{  Jarque-Bera (JB):  } & 830216.248  \\
\textbf{Skew:}          &  15.429 & \textbf{  Prob(JB):          } &      0.00   \\
\textbf{Kurtosis:}      & 258.000 & \textbf{  Cond. No.          } &      11.9   \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{center}

Notes: \newline
 [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.